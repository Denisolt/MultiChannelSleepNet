Title;Author;Dataset;Objective;Implementation;Results;State of the Art;Additional Notes
SLEEPNET: Automated Sleep Staging System via Deep Learning;Biswal, 2017;largest in the world. Massachusetts General Hospital Sleep Laboratory;Classification of the sleep stages(W N1 N2 N3 REM);RNN with LSTM cells, CNN and CRNN with a Spectogram as an input;Multilayer RNN with expert-defined features has the best performance, although RCNN also has very competitive performance. RNN = 85.76, RCNN = 81.67;;
A-phases subtype detection using different classification methods;Machado, 2017;CAP Sleep Database;Classification of the sleep stages;Support Vector Machine, k-Nearest Neighbour and Linear Discriminant Analysis;SVM = 71%, LDA = 69%, k-NN = 70%;;
"A deep learning architecture for temporal sleep stage classificationusing multivariate and multimodal time series";Chambon, 2017;Montreal Archive of Sleep Studies (MASS);Classification of the sleep stages(W N1 N2 N3 REM);Multivariate Network with EEG/EOG and EMG inputs separated, individual Spatial Filtering and Conv/Relu/, uniting at the softmax for classification;Random Sampling: Univariate 81%, Multivariate 84% | Balanced Sampling: Uni 70%, Multi 77%;Gradient boosting classifier(RS) Uni 75%, Multi 78% Tsinalies(2016) Balanced Sampling Only Uni 74%;balanced sampling, i.e. to feed the network with batches of data which contain as many data points from each class. Balanced sampling should be used to optimize the balanced accuracy of the model. On the other hand, random sampling should be used to boost the accuracy.
Automatic Sleep Stage Scoring with Single-Channel EEG Using Convolutional Neural Networks;Tsinalis, 2016;PSG dataset from PhysioNet;Classification of the sleep stages(W N1 N2 N3 REM);CNN architecture with input: 1D signal of length 15000 at 100Hz;mean accuracy - 82%;86% Tsinalis paper 2015;
DeepSleepNet: a Model for Automatic Sleep Stage Scoring based on Raw Single-Channel EE;Supratak, 2017;MASS, EDF;Classification of the sleep stages(W N1 N2 N3 REM);2 parts, Representation Learning, and Sequancial Residual Learning. 2 CNN, then bidirectional RNN with - LSTM cells;MASS 86.2%, Sleep - EDF 82%;12, 8, 9, 7 (Tsinalis 2016), 11(Tsinalis paper Single 2016), 13;
LEARNING REPRESENTATIONS FROM EEG WITH DEEP RECURRENT-CONVOLUTIONAL NEURAL NETWORKS;Bashivan, 2016;Private dataset;Classification of the Memory Task load;FFT, Polar Projection, CNN with LSTM using generated images as input;Test Error - 8.89%;;
Deep learning with convolutional neural networks for brain mapping and decoding of movement-related information from the human EEG;Schirrmeister, 2017;BCI Competition Dataset;Movement of the Hands, Feet, Rest;CNN with Raw EEG Data as input, using ELU, MaxPool;89.8%;;